{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMLwGiK8AU0+UuDmg8DGPzp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alwiyahya99/MachineLearning/blob/main/5_Klasifikasi_Gambar_Dengan_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Klasifikasi Gambar : Transfer Learning**"
      ],
      "metadata": {
        "id": "iBtIua-o85o6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ft2Ysg6yHZx",
        "outputId": "4e6c1587-50be-4bb3-aa15-169142a7e6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-19 09:07:17--  https://github.com/dicodingacademy/assets/raw/main/ml_pengembangan_academy/Chessman-image-dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dicodingacademy/assets/main/ml_pengembangan_academy/Chessman-image-dataset.zip [following]\n",
            "--2024-01-19 09:07:18--  https://raw.githubusercontent.com/dicodingacademy/assets/main/ml_pengembangan_academy/Chessman-image-dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60684125 (58M) [application/zip]\n",
            "Saving to: ‘/tmp/Chessman-image-dataset.zip’\n",
            "\n",
            "/tmp/Chessman-image 100%[===================>]  57.87M   171MB/s    in 0.3s    \n",
            "\n",
            "2024-01-19 09:07:18 (171 MB/s) - ‘/tmp/Chessman-image-dataset.zip’ saved [60684125/60684125]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://github.com/dicodingacademy/assets/raw/main/ml_pengembangan_academy/Chessman-image-dataset.zip \\\n",
        "  -O /tmp/Chessman-image-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "local_zip = '/tmp/Chessman-image-dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "YGXEXkC1BJnR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join('/tmp/Chessman-image-dataset/Chess')\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    fill_mode = 'nearest',\n",
        "    validation_split=0.1) # set validation split"
      ],
      "metadata": {
        "id": "NmeCdaZFBPpi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=8,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, # same directory as training data\n",
        "    target_size=(150, 150),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1WDPtlxBR2F",
        "outputId": "129c1516-7bc7-4088-a6fa-35fd849da462"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 499 images belonging to 6 classes.\n",
            "Found 52 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import ResNet152V2\n",
        "model = tf.keras.models.Sequential([\n",
        "    ResNet152V2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(150, 150, 3))),\n",
        "    # tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQWU4BIfDaci",
        "outputId": "758becc7-e1ea-4b2f-8063-5fb5dea5fe49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234545216/234545216 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "eLaavcYHDjQz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                              validation_data=validation_generator,\n",
        "                              epochs=50,\n",
        "                              verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDdA0vxMDmJ2",
        "outputId": "13e306e9-8ee8-44e4-b013-6dc802f925dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "63/63 - 174s - loss: 10.8087 - accuracy: 0.4389 - val_loss: 4.2541 - val_accuracy: 0.4808 - 174s/epoch - 3s/step\n",
            "Epoch 2/50\n",
            "63/63 - 169s - loss: 2.2477 - accuracy: 0.6393 - val_loss: 1.2722 - val_accuracy: 0.7115 - 169s/epoch - 3s/step\n",
            "Epoch 3/50\n",
            "63/63 - 160s - loss: 1.4174 - accuracy: 0.7435 - val_loss: 1.2106 - val_accuracy: 0.7115 - 160s/epoch - 3s/step\n",
            "Epoch 4/50\n",
            "63/63 - 159s - loss: 0.8633 - accuracy: 0.7996 - val_loss: 1.3301 - val_accuracy: 0.6923 - 159s/epoch - 3s/step\n",
            "Epoch 5/50\n",
            "63/63 - 159s - loss: 0.6368 - accuracy: 0.8437 - val_loss: 1.6056 - val_accuracy: 0.6346 - 159s/epoch - 3s/step\n",
            "Epoch 6/50\n",
            "63/63 - 157s - loss: 0.5660 - accuracy: 0.8437 - val_loss: 1.1393 - val_accuracy: 0.6731 - 157s/epoch - 2s/step\n",
            "Epoch 7/50\n",
            "63/63 - 157s - loss: 0.6309 - accuracy: 0.8437 - val_loss: 0.9957 - val_accuracy: 0.6538 - 157s/epoch - 2s/step\n",
            "Epoch 8/50\n",
            "63/63 - 159s - loss: 0.5769 - accuracy: 0.8317 - val_loss: 1.4661 - val_accuracy: 0.7885 - 159s/epoch - 3s/step\n",
            "Epoch 9/50\n",
            "63/63 - 171s - loss: 0.5709 - accuracy: 0.8717 - val_loss: 1.2291 - val_accuracy: 0.7308 - 171s/epoch - 3s/step\n",
            "Epoch 10/50\n",
            "63/63 - 169s - loss: 0.3272 - accuracy: 0.9118 - val_loss: 0.4732 - val_accuracy: 0.8654 - 169s/epoch - 3s/step\n",
            "Epoch 11/50\n",
            "63/63 - 160s - loss: 0.3617 - accuracy: 0.8838 - val_loss: 1.0786 - val_accuracy: 0.6923 - 160s/epoch - 3s/step\n",
            "Epoch 12/50\n",
            "63/63 - 160s - loss: 0.4055 - accuracy: 0.8958 - val_loss: 1.0536 - val_accuracy: 0.8269 - 160s/epoch - 3s/step\n",
            "Epoch 13/50\n",
            "63/63 - 160s - loss: 0.3801 - accuracy: 0.8898 - val_loss: 0.9901 - val_accuracy: 0.7500 - 160s/epoch - 3s/step\n",
            "Epoch 14/50\n",
            "63/63 - 170s - loss: 0.4088 - accuracy: 0.9098 - val_loss: 1.5595 - val_accuracy: 0.7692 - 170s/epoch - 3s/step\n",
            "Epoch 15/50\n",
            "63/63 - 171s - loss: 0.5276 - accuracy: 0.8978 - val_loss: 1.3261 - val_accuracy: 0.6731 - 171s/epoch - 3s/step\n",
            "Epoch 16/50\n",
            "63/63 - 162s - loss: 0.3860 - accuracy: 0.9158 - val_loss: 1.1809 - val_accuracy: 0.6923 - 162s/epoch - 3s/step\n",
            "Epoch 17/50\n",
            "63/63 - 169s - loss: 0.3704 - accuracy: 0.9178 - val_loss: 0.9833 - val_accuracy: 0.7500 - 169s/epoch - 3s/step\n",
            "Epoch 18/50\n",
            "63/63 - 167s - loss: 0.3138 - accuracy: 0.9259 - val_loss: 1.3402 - val_accuracy: 0.7115 - 167s/epoch - 3s/step\n",
            "Epoch 19/50\n",
            "63/63 - 161s - loss: 0.2256 - accuracy: 0.9419 - val_loss: 0.6312 - val_accuracy: 0.7885 - 161s/epoch - 3s/step\n",
            "Epoch 20/50\n",
            "63/63 - 167s - loss: 0.2186 - accuracy: 0.9319 - val_loss: 0.5067 - val_accuracy: 0.7500 - 167s/epoch - 3s/step\n",
            "Epoch 21/50\n",
            "63/63 - 164s - loss: 0.2367 - accuracy: 0.9339 - val_loss: 1.6589 - val_accuracy: 0.6923 - 164s/epoch - 3s/step\n",
            "Epoch 22/50\n",
            "63/63 - 163s - loss: 0.2683 - accuracy: 0.9138 - val_loss: 0.9454 - val_accuracy: 0.7692 - 163s/epoch - 3s/step\n",
            "Epoch 23/50\n",
            "63/63 - 163s - loss: 0.2262 - accuracy: 0.9459 - val_loss: 1.3987 - val_accuracy: 0.6923 - 163s/epoch - 3s/step\n",
            "Epoch 24/50\n",
            "63/63 - 162s - loss: 0.2533 - accuracy: 0.9299 - val_loss: 1.3016 - val_accuracy: 0.7692 - 162s/epoch - 3s/step\n",
            "Epoch 25/50\n",
            "63/63 - 163s - loss: 0.2920 - accuracy: 0.9319 - val_loss: 1.1319 - val_accuracy: 0.7692 - 163s/epoch - 3s/step\n",
            "Epoch 26/50\n",
            "63/63 - 164s - loss: 0.1656 - accuracy: 0.9619 - val_loss: 1.3850 - val_accuracy: 0.7308 - 164s/epoch - 3s/step\n",
            "Epoch 27/50\n",
            "63/63 - 164s - loss: 0.2797 - accuracy: 0.9339 - val_loss: 1.4827 - val_accuracy: 0.7115 - 164s/epoch - 3s/step\n",
            "Epoch 28/50\n",
            "63/63 - 162s - loss: 0.3609 - accuracy: 0.9198 - val_loss: 0.6447 - val_accuracy: 0.8269 - 162s/epoch - 3s/step\n",
            "Epoch 29/50\n",
            "63/63 - 164s - loss: 0.1763 - accuracy: 0.9399 - val_loss: 0.8290 - val_accuracy: 0.7885 - 164s/epoch - 3s/step\n",
            "Epoch 30/50\n",
            "63/63 - 163s - loss: 0.1545 - accuracy: 0.9579 - val_loss: 1.1436 - val_accuracy: 0.7885 - 163s/epoch - 3s/step\n",
            "Epoch 31/50\n",
            "63/63 - 154s - loss: 0.1168 - accuracy: 0.9619 - val_loss: 1.2084 - val_accuracy: 0.7500 - 154s/epoch - 2s/step\n",
            "Epoch 32/50\n",
            "63/63 - 162s - loss: 0.1009 - accuracy: 0.9800 - val_loss: 0.7055 - val_accuracy: 0.8269 - 162s/epoch - 3s/step\n",
            "Epoch 33/50\n",
            "63/63 - 155s - loss: 0.0548 - accuracy: 0.9820 - val_loss: 0.8884 - val_accuracy: 0.8269 - 155s/epoch - 2s/step\n",
            "Epoch 34/50\n",
            "63/63 - 163s - loss: 0.1831 - accuracy: 0.9539 - val_loss: 1.7506 - val_accuracy: 0.6923 - 163s/epoch - 3s/step\n",
            "Epoch 35/50\n",
            "63/63 - 154s - loss: 0.1385 - accuracy: 0.9619 - val_loss: 1.0325 - val_accuracy: 0.7692 - 154s/epoch - 2s/step\n",
            "Epoch 36/50\n",
            "63/63 - 154s - loss: 0.0665 - accuracy: 0.9820 - val_loss: 0.2984 - val_accuracy: 0.9231 - 154s/epoch - 2s/step\n",
            "Epoch 37/50\n",
            "63/63 - 163s - loss: 0.0816 - accuracy: 0.9800 - val_loss: 0.7217 - val_accuracy: 0.8077 - 163s/epoch - 3s/step\n",
            "Epoch 38/50\n",
            "63/63 - 162s - loss: 0.0920 - accuracy: 0.9739 - val_loss: 0.5869 - val_accuracy: 0.8077 - 162s/epoch - 3s/step\n",
            "Epoch 39/50\n",
            "63/63 - 153s - loss: 0.1111 - accuracy: 0.9679 - val_loss: 0.6196 - val_accuracy: 0.8077 - 153s/epoch - 2s/step\n",
            "Epoch 40/50\n",
            "63/63 - 154s - loss: 0.1901 - accuracy: 0.9499 - val_loss: 0.7630 - val_accuracy: 0.8269 - 154s/epoch - 2s/step\n",
            "Epoch 41/50\n",
            "63/63 - 155s - loss: 0.1791 - accuracy: 0.9619 - val_loss: 0.6148 - val_accuracy: 0.8077 - 155s/epoch - 2s/step\n",
            "Epoch 42/50\n",
            "63/63 - 163s - loss: 0.1007 - accuracy: 0.9699 - val_loss: 1.0704 - val_accuracy: 0.7308 - 163s/epoch - 3s/step\n",
            "Epoch 43/50\n",
            "63/63 - 163s - loss: 0.1174 - accuracy: 0.9699 - val_loss: 0.7464 - val_accuracy: 0.8077 - 163s/epoch - 3s/step\n",
            "Epoch 44/50\n",
            "63/63 - 155s - loss: 0.0976 - accuracy: 0.9659 - val_loss: 0.6953 - val_accuracy: 0.8269 - 155s/epoch - 2s/step\n",
            "Epoch 45/50\n",
            "63/63 - 164s - loss: 0.0767 - accuracy: 0.9780 - val_loss: 0.7606 - val_accuracy: 0.8269 - 164s/epoch - 3s/step\n",
            "Epoch 46/50\n",
            "63/63 - 163s - loss: 0.1722 - accuracy: 0.9499 - val_loss: 0.4362 - val_accuracy: 0.8462 - 163s/epoch - 3s/step\n",
            "Epoch 47/50\n",
            "63/63 - 161s - loss: 0.0925 - accuracy: 0.9679 - val_loss: 1.0210 - val_accuracy: 0.8846 - 161s/epoch - 3s/step\n",
            "Epoch 48/50\n",
            "63/63 - 161s - loss: 0.0217 - accuracy: 0.9920 - val_loss: 0.6683 - val_accuracy: 0.8654 - 161s/epoch - 3s/step\n",
            "Epoch 49/50\n",
            "63/63 - 164s - loss: 0.1040 - accuracy: 0.9820 - val_loss: 0.8270 - val_accuracy: 0.7692 - 164s/epoch - 3s/step\n",
            "Epoch 50/50\n",
            "63/63 - 164s - loss: 0.3062 - accuracy: 0.9519 - val_loss: 3.1806 - val_accuracy: 0.6154 - 164s/epoch - 3s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8LtZz0OFpPrT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}